{
  "description": "是一个用于评估大型语言模型物理推理能力的基准测试。",
  "selectField": "选择领域",
  "selectScoreType": "选择分数类型",
  "eedScoreTitle": "EED 评分",
  "accScoreTitle": "准确率评分",
  "chartTitle": "模型性能",
  "chartDescription": "各模型得分",
  "leaderboardTitle": "模型排行榜",
  "leaderboardDescription": "模型得分排名",
  "rank": "排名",
  "model": "模型",
  "organization": "组织机构",
  "score": "得分",
  "performance": "性能",
  "detailedScores": "详细得分",
  "modelDetails": "模型性能详情",
  "modelInfo": "模型信息",
  "modelId": "模型ID",
  "scoreLabel": "得分",
  "force": "强制启用推理",
  "fields": {
    "ALL": "总体得分",
    "MECHANICS": "力学",
    "ELECTRICITY": "电磁学",
    "THERMODYNAMICS": "热力学",
    "OPTICS": "光学",
    "MODERN": "近代物理",
    "ADVANCED": "高等物理"
  },
  "news": {
    "post": {
      "grok4": "Grok4创造物理推理新纪录，距人类专家水平仍有差距 PHYBench团队发布Grok4最新评测结果： Grok4          ACC: 42.33   EED: 53.03 Gemini 2.5 Pro ACC: 36.87   EED: 49.46 Human Expert   ACC: 61.90   EED: 71.40 Grok4全面超越前SOTA模型Gemini 2.5 Pro，但仍明显落后于人类专家基准。 值得关注的是，Grok4在HLE上断崖领先其他模型，但在PHYBench上仅取得微弱优势，这凸显了PHYBench更能客观衡量模型真实能力。与此同时，国内大模型表现紧随其后，并未掉队。 北大物理学院将持续更新PHYBench，致力于为大模型物理推理评测提供最客观评判。"
    }
  }
}
