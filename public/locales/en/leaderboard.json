{
  "description": "is a benchmark for evaluating the physical reasoning capabilities of large language models.",
  "selectField": "Select Field",
  "selectScoreType": "Select Score Type",
  "eedScoreTitle": "EED Score",
  "accScoreTitle": "Accuracy Score",
  "chartTitle": "Model Performance",
  "chartDescription": "Comparison of models by score",
  "leaderboardTitle": "Model Leaderboard",
  "leaderboardDescription": "Ranking of models by performance",
  "rank": "Rank",
  "model": "Model",
  "organization": "Organization",
  "score": "Score",
  "performance": "Performance",
  "detailedScores": "Detailed Scores",
  "modelDetails": "Model Performance Details",
  "modelInfo": "Model Information",
  "modelId": "Model ID",
  "scoreLabel": "Score",
  "force": "Force enable reasoning",
  "fields": {
    "ALL": "Overall Score",
    "MECHANICS": "Mechanics",
    "ELECTRICITY": "Electricity",
    "THERMODYNAMICS": "Thermodynamics",
    "OPTICS": "Optics",
    "MODERN": "Modern",
    "ADVANCED": "Advanced"
  }
}
